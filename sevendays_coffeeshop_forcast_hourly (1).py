# -*- coding: utf-8 -*-
"""SevenDays_CoffeeShop_Forcast_Hourly.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kxy5dViqSOt6ULVCSYRpqW6kD_LjiQ2k
"""

# Import necessary libraries
from google.colab import files
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit,cross_val_score
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import RobustScaler,StandardScaler
import seaborn as sns
from sklearn.cluster import KMeans
import plotly.express as px
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Set random seed for reproducibility
np.random.seed(42)

# Load the CSV file
data = pd.read_csv('Project.csv')

data.head()

data['Size'].unique()

data.head()

data.info()

data.shape

data.isna().sum()

# Numerical columns summary
print("Numerical Columns Summary:")
data.describe()

# Categorical Columns Summary

print("\nCategorical Columns Summary:")
categorical_columns = data.select_dtypes(include=['object']).columns
data[categorical_columns].describe()

data.duplicated().sum()

def plot_distributions(data, numerical_cols, rows=3, cols=3):
    plt.figure(figsize=(15, 4*rows))
    for i, col in enumerate(numerical_cols, 1):
        plt.subplot(rows, cols, i)
        sns.histplot(data[col], kde=True)
        plt.title(f'Distribution of {col}')
        plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_distributions(data, ['unit_price','Total_Bill'])

# Box plots for numerical columns
numerical_cols=['unit_price','Total_Bill','transaction_qty']
plt.figure(figsize=(15, 4))
sns.boxplot(data=data[numerical_cols])
plt.xticks(rotation=45)
plt.title('Outlier Detection using Box Plots')
plt.show()

# Calculate IQR and identify outliers
def identify_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
    return len(outliers)

for col in numerical_cols:
    n_outliers = identify_outliers(data, col)
    print(f"Number of outliers in {col}: {n_outliers}")

data['Size'].value_counts()

# 1. Function to handle outliers using IQR method
def handle_outliers(df, column, method='clip'):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    if method == 'clip':
        return df[column].clip(lower=lower_bound, upper=upper_bound)
    elif method == 'remove':
        return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# For unit_price (4212 outliers) - more aggressive treatment
data['unit_price_cleaned'] = handle_outliers(data, 'unit_price', method='clip')

# For Total_Bill (3273 outliers) - more aggressive treatment
data['Total_bill_cleaned'] = handle_outliers(data, 'Total_Bill', method='clip')

sorted_product_types = data['product_type'].value_counts().sort_values().index

# Create the count plot with sorted product types
plt.figure(figsize=(12, 8))
sns.countplot(y=data['product_type'], order=sorted_product_types)
plt.title('Count Plot of Product Types (Sorted)')
plt.xlabel('Count')
plt.ylabel('Product Type')
plt.show()

sorted_data = data.sort_values(by='unit_price')

# Create the bar plot with sorted data and error bars removed
plt.figure(figsize=(12, 8))
sns.barplot(data=sorted_data, x='unit_price', y='product_type', errorbar=None)
plt.title('Bar Plot of Unit Price by Product Type ')
plt.xlabel('Unit Price')
plt.ylabel('Product Type')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data['store_location'])
plt.title('Distribution of Store Locations')
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""#Data Preprocessing"""

scaler_total_bill = RobustScaler().fit(data[['Total_Bill']])
scaler_unit_price = RobustScaler().fit(data[['unit_price']])
data['robust_scaled_Total_bill'] = scaler_total_bill.transform(data[['Total_Bill']])
data['robust_scaled_unit_price'] = scaler_unit_price.transform(data[['unit_price']])

# Data Preprocessing
data['transaction_datetime'] = pd.to_datetime(data['transaction_date'] + ' ' + data['transaction_time'], format='%d-%m-%Y %H:%M:%S')
data = data.sort_values('transaction_datetime')

# Aggregate data hourly
data['date_hour'] = data['transaction_datetime'].dt.floor('h')

hourly_data = data.groupby('date_hour').agg({
    'transaction_qty': 'sum',
    'robust_scaled_unit_price': 'mean',
    'unit_price_cleaned':'mean',
    'unit_price':'mean',
    'Total_Bill':'sum',
    'Total_bill_cleaned':'sum',
    'robust_scaled_Total_bill': 'sum'
}).reset_index()

# Create additional time-based features
hourly_data['day_of_week'] = hourly_data['date_hour'].dt.dayofweek
hourly_data['hour'] = hourly_data['date_hour'].dt.hour
hourly_data['day_of_month'] = hourly_data['date_hour'].dt.day
hourly_data['month'] = hourly_data['date_hour'].dt.month
hourly_data['year'] = hourly_data['date_hour'].dt.year

hourly_data['lag_2']=hourly_data['transaction_qty'].shift(2)
hourly_data['Mean_Roll'] = hourly_data['transaction_qty'].rolling(window=2, center=True).mean()
hourly_data['Max_Roll'] = hourly_data['transaction_qty'].rolling(window=3).max()
hourly_data['Max_Expand'] = hourly_data['transaction_qty'].expanding().max()

decomposition = seasonal_decompose(hourly_data['transaction_qty'], model='additive', period=365)
hourly_data['seasonal'] = decomposition.seasonal
hourly_data.fillna(method='bfill', inplace=True)

hourly_data.set_index(['date_hour'],inplace=True)

hourly_data.head(48)

"""#### Correlation of Hourly_data"""

plt.figure(figsize=(12, 8))
sns.heatmap(hourly_data.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""#Feature Engineering"""

# Define features (X) and target (y)
X = hourly_data[['hour', 'day_of_month','lag_2', 'month','Mean_Roll','seasonal', 'robust_scaled_unit_price', 'robust_scaled_Total_bill']]
X_F = hourly_data[['day_of_week', 'hour','lag_2','Mean_Roll','seasonal','day_of_month', 'month','robust_scaled_Total_bill']]

y = hourly_data['transaction_qty']



X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)
X_train_F,X_test_F,y_train_F,y_test_F=train_test_split(X_F,y,test_size=0.2)

"""#Modeling and Hypertuning"""

param_grid = {
    'n_estimators': [550],
    'learning_rate': [0.1],
    'max_depth': [4],
    'subsample': [0.8]
}
model = GradientBoostingRegressor(random_state=42)

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Cross-Validation Score:", best_score)

best_model = grid_search.best_estimator_

y1_pred = best_model.predict(X_test)
mseg = mean_squared_error(y_test, y1_pred)
R2g = r2_score(y_test, y1_pred)
maeg = mean_absolute_error(y_test, y1_pred)
print("R2_score:", R2g)
print("Mean Squared Error:", mseg)
print("Mean Absolute Error:", maeg)

# Forecasting the Next 7 Days Hourly
start_time = hourly_data.index[-1] + pd.Timedelta(hours=1)

# Generate a range of 24*7 hours
all_hours = pd.date_range(start=start_time, periods=24*7, freq='h')

# Filter to include only hours 0 to 19 each day (20 hours per day)
future_dates = all_hours[( all_hours.hour >= hourly_data.index.hour.min() ) & ( all_hours.hour <= hourly_data.index.hour.max())]


mean_Total_bill = hourly_data['robust_scaled_Total_bill'].tail(15*7)
mean_roll=hourly_data['Mean_Roll'].tail(15*7)
max_roll=hourly_data['Max_Roll'].tail(15*7)
max_expand=hourly_data['Max_Expand'].tail(15*7)
lag_2=hourly_data['lag_2'].tail(15*7)
seasonal=hourly_data['seasonal'].tail(15*7)
Unite_Price=hourly_data['robust_scaled_unit_price'].tail(15*7)

# Create a DataFrame for future dates with the same features
future_df = pd.DataFrame({
    'date_hour': future_dates,
    'day_of_week': future_dates.dayofweek,
    'hour': future_dates.hour,
    'day_of_month': future_dates.day,
    'month': future_dates.month,
    'year': future_dates.year,
    'robust_scaled_Total_bill':mean_Total_bill,
    'seasonal':seasonal,
    'lag_2':lag_2,
    'Max_Roll':max_roll,
    'Max_Expand':max_expand,
    'Mean_Roll':mean_roll,
    'robust_scaled_unit_price':Unite_Price
    })

# Get feature importances
feature_importances = best_model.feature_importances_

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.gca().invert_yaxis()
plt.show()

# Print the feature importances
print(importance_df)

columns=['day_of_week','hour','lag_2','Mean_Roll','seasonal','day_of_month','month','robust_scaled_Total_bill']

y

# Make predictions
best_model.fit(X_train_F,y_train)
future_predictions = best_model.predict(future_df[['day_of_week', 'hour','lag_2','Mean_Roll','seasonal','day_of_month', 'month','robust_scaled_Total_bill']])
# Ensure positive predictions
future_predictions = np.maximum(future_predictions, 0)

forecast_df = future_df.copy()
forecast_df['forecast'] = future_predictions.astype(int)

future_df.columns

forecast_df.info()

# Visualize the forecast
fig = px.line(hourly_data, x=hourly_data.index, y='transaction_qty', title='Hourly Transaction Quantity with 7-Day Forecast')
fig.add_scatter(x=forecast_df['date_hour'], y=forecast_df['forecast'], mode='lines', name='Forecast for The cups')
fig.update_layout(title_text='Hourly Transaction Quantity with 7-Day Forecast', title_x=0.5)
fig.show()

Combined=pd.DataFrame(dict({'date_hour':forecast_df['date_hour'],'Forecast':forecast_df['forecast']}))

Combined.to_csv('combined_forecast.csv',index=False)

# Get feature importances
feature_importances = best_model.feature_importances_

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({
    'Feature': future_df[columns].columns,
    'Importance': feature_importances
})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.gca().invert_yaxis()
plt.show()

# Print the feature importances
print(importance_df)

def calculate_vif(df):
    vif_data = pd.DataFrame()
    vif_data['feature'] = df.columns
    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]
    return vif_data

calculate_vif(X)



# Load the combined forecast data
combined_df = pd.read_csv('combined_forecast.csv')

# Calculate the required coffee beans in grams
# Equation: coffee_beans_grams = transaction_qty * 15
combined_df['coffee_beans_grams'] = combined_df['Forecast'] * 15

# Save the updated DataFrame with coffee beans calculation to a new CSV file
combined_df_with_beans = 'combined_forecast_with_beans.csv'
combined_df.to_csv(combined_df_with_beans, index=False)

combined_df

# Visualize the forecast and the required coffee beans using Plotly
fig = px.line(combined_df, x='date_hour', y='Forecast', title='Hourly Transaction Quantity with 7-Day Forecast')
fig.add_scatter(x=combined_df['date_hour'], y=combined_df['coffee_beans_grams'], mode='lines', name='Required Coffee Beans (grams)')
fig.add_scatter(x=combined_df['date_hour'], y=combined_df['Forecast'], mode='lines', name='Required Coffee Cups')
fig.update_layout(title_text='Hourly Transaction quantity ,Coffee Beans and Coffee Cups with 7-day Forcast', title_x=0.5)

# Display the figure (optional)
fig.show()

forecast_df.reset_index(drop=True,inplace=True)

forecast_df

forecast_df.set_index(['date_hour'], inplace=True)

wcss = []  # Store Within-Cluster Sum of Squares
K_range = range(1, 10)  # Test k from 1 to 9

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(forecast_df.iloc[:,1:])  # Exclude hour column
    wcss.append(kmeans.inertia_)  # Store WCSS value

# Plot Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(K_range, wcss, marker='o', linestyle='--')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.title('Elbow Method for Optimal k')
plt.show()

kmeans = KMeans(n_clusters=3, random_state=42)
forecast_df['Cluster'] = kmeans.fit_predict(forecast_df.iloc[:,2:])

# Map clusters to actual cup types based on proportions
# Assuming, after inspecting data, cluster 0 -> Large, 1 -> Regular, 2 -> Small
cluster_map = {
    0: 'Large',
    1: 'Regular',
    2: 'Small'
}

# Add the predicted cup type to the dataframe
forecast_df['Predicted Cup Type'] = forecast_df['Cluster'].map(cluster_map)

forecast_df[['forecast','Predicted Cup Type']]



# Visualize the forecast and the required coffee beans using Plotly
fig = px.line(forecast_df, x=forecast_df.index, y='forecast', title='Hourly Transaction Quantity with 7-Day Forecast')
fig.add_scatter(x=forecast_df.index, y=forecast_df['forecast'], mode='lines', name='Required Coffee Cups', line=dict(color='green'))
fig.update_layout(title_text='Coffee Cups with 7-day Forcast', xaxis_title='Date',
    yaxis_title='Cups Quantity',title_x=0.5)

# Display the figure (optional)
fig.show()

forecast_df_2=forecast_df[['Predicted Cup Type','forecast']]

forecast_json = forecast_df_2.to_json(orient='records', lines=True)

import openai
openai.api_key = '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'  # Replace with your actual API key
import streamlit as st
def get_forecasted_data():
    # Replace this with your actual data retrieval logic
    forecast_data =forecast_json
    return forecast_data
def ask_gpt3(question):
    forecast_data=get_forecasted_data()
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": question},
            {"role": "function", "name": "get_forecasted_data", "content": forecast_data}
        ]
    )
    return response['choices'][0]['message']['content']

# Example question
question = "get how many cup the will be utilized with their types for the next hour get accurately on the 7th july of 2023 providing recommendations to raise the sales"
answer = ask_gpt3(question)
print(answer)

# Initialize chat history in session state
if 'messages' not in st.session_state:
    st.session_state['messages'] = []

st.title("Weather Forecast Chatbot")

# Display chat history
for message in st.session_state['messages']:
    st.write(f"**{message['role'].capitalize()}:** {message['content']}")

# User input
user_input = st.text_input("You:", key="user_input")

if user_input:
    # Append user message to chat history
    st.session_state['messages'].append({"role": "user", "content": user_input})

    # Generate response from GPT-4
    assistant_response = ask_gpt3(user_input)

    # Append assistant response to chat history
    st.session_state['messages'].append({"role": "assistant", "content": assistant_response})

    # Clear input box
    st.session_state.user_input = ""

    # Display updated chat history
    st.experimental_rerun()

!streamlit run app.py

